# -*- coding: utf-8 -*-
"""ml_logger.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DHs3kVl9L9v6SCT6X1UmD6pH2toIemuM
"""

import sys
import os
import datetime
import pandas as pd
import pytz
from sklearn.model_selection import ParameterGrid
import json
import pickle
import uuid



sys.path.append('/content/drive/MyDrive/utilize/')
from metrics import Metrics

class MLLogger:
    def __init__(self, log_file='ml_log.csv', artifact_dir = 'artifacts'):
        self.log_file = log_file
        self.log_df = None
        self._load_log()
        self.metric_names = []
        self.custom_metrics = {}

        self.metric_functions = {
            'accuracy': Metrics.evaluate_accuracy,
            'f1_score': Metrics.evaluate_f1_score,
            'roc_auc': Metrics.evaluate_roc_auc,
            'confusion_matrix': Metrics.evaluate_confusion_matrix
            # Add more metrics here if needed
        }

        self.artifact_dir = artifact_dir  # Directory to store artifacts
        os.makedirs(self.artifact_dir, exist_ok=True)

        self._load_log()

    def _generate_unique_id(self):
      return str(uuid.uuid4())  # Generate a unique identifier


    def _load_log(self):
        if os.path.exists(self.log_file):
            self.log_df = pd.read_csv(self.log_file)
            self.metric_names = [col for col in self.log_df.columns if col not in ['Date', 'Model', 'Parameters']]

    def add_custom_metric(self, name, function):
            """
            Add a custom metric to the logger.
            Args:
                name (str): Name of the custom metric.
                function (callable): Custom metric function.
            """
            self.custom_metrics[name] = function

    """
    def _log_metrics(self, model_name, params, metric_results):
        now = datetime.datetime.now()
        date_str = now.strftime('%Y-%m-%d %H:%M:%S')

        new_rows = []
        for metric_name, values in metric_results.items():
            new_row = {'Date': date_str, 'Model': model_name, 'Parameters': str(params), 'Metric': metric_name}
            new_row.update(values)
            new_rows.append(new_row)

        self.log_df = self.log_df.append(new_rows, ignore_index=True)
        self.log_df.to_csv(self.log_file, index=False)
    """
    def _log_metrics(self, model, metric_results):
        now = datetime.datetime.now(pytz.timezone('Europe/Istanbul'))  # Set the timezone to Turkish time
        date_str = now.strftime('%Y-%m-%d %H:%M:%S')

        model_name = getattr(model, 'model_name', str(model.__class__).split('.')[-1].rstrip("'>"))
        params = model.get_params()
        params_str = "\n".join([f"{key}={value}" if len(f"{key}={value}") <= 50 else f"{key}={value[:50]}...\n..." for key, value in params.items() if value is not None])
        
        unique_id = self._generate_unique_id()
        artifact_name_with_id = f"{model_name}_{unique_id}"

        if not self.metric_names:
            self.metric_names = list(metric_results.keys())
            columns = ['ID', 'Date', 'Model', 'Parameters'] + [f"{split}_{metric}" for split in ['train', 'test'] for metric in self.metric_names]
            self.log_df = pd.DataFrame(columns=columns)

        new_row = {'ID': artifact_name_with_id,'Date': date_str, 'Model': model_name, 'Parameters': params_str}
        for metric_name, values in metric_results.items():
            for split in ['train', 'test']:
                new_row[f"{split}_{metric_name}"] = values[split.lower()]

        self.log_df = self.log_df.append(new_row, ignore_index=True)
        self.log_df.to_csv(self.log_file, index=False)

    def fit_and_log(self,model, X_train, y_train, X_test, y_test,metrics=['accuracy', 'f1_score', 'roc_auc', 'confusion_matrix'],  custom_metric_names=None):
        model.fit(X_train, y_train)
        train_predictions = model.predict(X_train)
        test_predictions = model.predict(X_test)
        test_pred_prob = model.predict_proba(X_test)  # For ROC AUC

        metric_results = {}

        if custom_metric_names:
            for name in custom_metric_names:
                metric_func = self.custom_metrics[name]
                print(metric_func)
                train_metric = metric_func(y_train, train_predictions)
                test_metric = metric_func(y_test, test_predictions)
                metric_results[name] = {'train': train_metric, 'test': test_metric}

        for metric_name in metrics:
            if metric_name in self.metric_functions:
                metric_func = self.metric_functions[metric_name]
                if metric_name == 'roc_auc':
                    #test_metric = metric_func(y_test, test_pred_prob)
                    #metric_results[metric_name] = {'test': test_metric}
                    pass
                elif metric_name == 'confusion_matrix':
                    confusion_mat_tr = metric_func(y_train, train_predictions)
                    confusion_mat_te = metric_func(y_test, test_predictions)
                    metric_results[metric_name] = {'train': confusion_mat_tr, 'test': confusion_mat_te}
                else:
                    train_metric = metric_func(y_train, train_predictions)
                    test_metric = metric_func(y_test, test_predictions)
                    metric_results[metric_name] = {'train': train_metric, 'test': test_metric}
            else:
                raise ValueError(f"Invalid metric specified: {metric_name}")

        self._log_metrics(model, metric_results)

    def fit_and_log_with_hyperparams(self, model, hyperparameter_dict, X_train, y_train, X_test, y_test, metrics=['accuracy', 'f1_score', 'roc_auc']):
      hyperparameter_grid = ParameterGrid(hyperparameter_dict)

      for config in hyperparameter_grid:
          model.set_params(**config)  # Set the hyperparameters

          model_name = getattr(model, 'model_name', str(model.__class__).split('.')[-1].rstrip("'>"))
          params = model.get_params()
          params_str = ", ".join([f"{key}={value}" for key, value in params.items() if value is not None])

          train_predictions = model.fit(X_train, y_train).predict(X_train)
          test_predictions = model.predict(X_test)
          test_pred_prob = model.predict_proba(X_test)

          metric_results = {}
          for metric_name in metrics:
            if metric_name in self.metric_functions:
                metric_func = self.metric_functions[metric_name]
                if metric_name == 'roc_auc':
                    #test_metric = metric_func(y_test, test_pred_prob)
                    #metric_results[metric_name] = {'test': test_metric}
                    pass
                elif metric_name == 'confusion_matrix':
                    confusion_mat_tr = metric_func(y_train, train_predictions)
                    confusion_mat_te = metric_func(y_test, test_predictions)
                    metric_results[metric_name] = {'train': confusion_mat_tr, 'test': confusion_mat_te}
                else:
                    train_metric = metric_func(y_train, train_predictions)
                    test_metric = metric_func(y_test, test_predictions)
                    metric_results[metric_name] = {'train': train_metric, 'test': test_metric}
            else:
                raise ValueError(f"Invalid metric specified: {metric_name}")

          self._log_metrics(model, metric_results)


    def print_log(self):
        print(self.log_df)

    def save_model_artifact(self, model, artifact_name):
      artifact_path = os.path.join(self.artifact_dir, artifact_name)
      with open(artifact_path, 'wb') as artifact_file:
          pickle.dump(model, artifact_file)
    
    def select_best_model(self, metric='accuracy', split='test', ascending=False):
        if self.log_df is None:
            raise ValueError("No log data available. Use the log_model_selection method to add entries.")

        best_model_entry = self.log_df.sort_values(by=[f'{split}_{metric}', f'train_{metric}'], ascending=ascending).iloc[0]
        best_model_artifact_name = best_model_entry['ID']

        best_model_name = best_model_entry['Model']
        best_model_params = best_model_entry['Parameters']

        best_model_params = best_model_params.replace('\n', ' ')
        best_model_info = {
            'best_model_id': best_model_artifact_name,
            'best_model_name': best_model_name,
            'best_model_params': best_model_params
        }

        return best_model_info


    